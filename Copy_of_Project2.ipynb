{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mansikhobragade/Helpdesk/blob/main/Copy_of_Project2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8OgpZzcIMpx"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykjWfWJ8JJcH"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "748G0Z4kmL5v"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWfqSI7Hlv-t"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7wgrnk2JtiL"
      },
      "outputs": [],
      "source": [
        "zip_file_path = ('/content/drive/MyDrive/B. Disease Grading.zip')\n",
        "target_folder = ('/content/drive/MyDrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUO3fWkyOuRx"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(zip_file_path,'r') as zip_ref:\n",
        "  zip_ref.extractall(target_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QySRSqfpYPg-"
      },
      "outputs": [],
      "source": [
        "extracted_folder = os.path.splitext(zip_file_path)[0] #Remove the .zip extension\n",
        "contents = os.listdir(extracted_folder)\n",
        "print(\"contents of the extracted folder:\")\n",
        "for item in contents:\n",
        "  print (item)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kI0IeNbId8p3"
      },
      "outputs": [],
      "source": [
        "groundtruth_folder = os.path.join(extracted_folder, '2. Groundtruths')\n",
        "groundtruth_contents = os.listdir(groundtruth_folder)\n",
        "\n",
        "print(\"\\nFiles under 'Groundtruths':\")\n",
        "print(groundtruth_contents)\n",
        "for csv_file in groundtruth_contents:\n",
        "  if csv_file.endswith('.csv'):\n",
        "    csv_path = os.path.join(groundtruth_folder,csv_file)\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(f\"\\nHead of '{csv_file}':\")\n",
        "    print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2VsxR7pfpOF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fgTloyPgRjl"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io\n",
        "from skimage.transform import resize\n",
        "from imgaug import augmenters as iaa\n",
        "from tqdm import tqdm\n",
        "import PIL\n",
        "from PIL import Image, ImageOps\n",
        "import cv2\n",
        "from sklearn.utils import class_weight, shuffle\n",
        "from keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "#from keras.applications.resnet50 import ResNet50\n",
        "#from keras.applications.resnet50 import preprocess_input\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import f1_score, fbeta_score\n",
        "from keras.utils import Sequence\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "WORKERS = 2\n",
        "CHANNEL = 3\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "IMG_SIZE = 512\n",
        "NUM_CLASSES = 5\n",
        "SEED = 77\n",
        "TRAIN_NUM = 1000 # use 1000 when you just want to explore new idea, use -1 for full train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gb4EpJNcgY13"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/B. Disease Grading/2. Groundtruths/a. IDRiD_Disease Grading_Training Labels.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/B. Disease Grading/2. Groundtruths/b. IDRiD_Disease Grading_Testing Labels.csv')\n",
        "\n",
        "x = df_train['Image name']\n",
        "y = df_train['Retinopathy grade']\n",
        "\n",
        "x, y = shuffle(x, y, random_state=SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4RdfMVhha60"
      },
      "outputs": [],
      "source": [
        "train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.20, stratify=y, random_state=SEED)\n",
        "print(train_x.shape, train_y.shape, valid_x.shape, valid_y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okUzlUiWhf4g"
      },
      "outputs": [],
      "source": [
        "train_y.hist()\n",
        "valid_y.hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQ1eMpxDfQ8f"
      },
      "outputs": [],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8etgLRqgHGr2"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "!pwd\n",
        "fig = plt.figure(figsize=(15, 16))\n",
        "# display 10 images from each class\n",
        "for class_id in sorted(train_y.unique()):\n",
        "    for i, (idx, row) in enumerate(df_train.loc[df_train['Retinopathy grade'] == class_id].sample(5, random_state=SEED).iterrows()):\n",
        "        ax = fig.add_subplot(5, 5, class_id * 5 + i + 1, xticks=[], yticks=[])\n",
        "        path=f\"/content/drive/MyDrive/B. Disease Grading/1. Original Images/a. Training Set/{row['Image name']}.jpg\"\n",
        "        image = cv2.imread(path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "        plt.imshow(image)\n",
        "        ax.set_title('Label: %d-%d-%s' % (class_id, idx, row['Image name']) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttGbgHqnjJLC"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the CLAHE parameters\n",
        "clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8, 8))\n",
        "\n",
        "# Specify the paths to the dataset\n",
        "dataset_path = \"/content/drive/MyDrive/B. Disease Grading\"\n",
        "train_images_path = os.path.join(dataset_path, \"1. Original Images/a. Training Set/\")\n",
        "train_csv_path = os.path.join(dataset_path, \"2. Groundtruths/a. IDRiD_Disease Grading_Training Labels.csv\")\n",
        "\n",
        "# Read the train CSV file\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "\n",
        "# Define the list of classes\n",
        "class_names = [\"class_0\", \"class_1\", \"class_2\", \"class_3\", \"class_4\"]\n",
        "\n",
        "# Preprocess 1 random image from each class\n",
        "num_samples = 2\n",
        "\n",
        "for class_name in class_names:\n",
        "    class_df = train_df[train_df[\"Retinopathy grade\"] == class_names.index(class_name)]\n",
        "    class_samples = class_df.sample(n=num_samples, random_state=42)\n",
        "\n",
        "    for _, row in class_samples.iterrows():\n",
        "        image_id = row[\"Image name\"]\n",
        "        image_path = os.path.join(train_images_path, image_id + \".jpg\")\n",
        "\n",
        "        # Load the image\n",
        "        image = cv2.imread(image_path, 0)  # Load as grayscale image\n",
        "\n",
        "        # Apply CLAHE\n",
        "        image_clahe = clahe.apply(image)\n",
        "\n",
        "        # Convert back to BGR color space for displaying with matplotlib\n",
        "        image_clahe_bgr = cv2.cvtColor(image_clahe, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "\n",
        "        # Plot the original and CLAHE processed images\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(5, 5))\n",
        "        axes[0].imshow(image, cmap=\"gray\")\n",
        "        axes[0].set_title(f\"Original img-{class_name}\")\n",
        "        axes[0].axis(\"off\")\n",
        "        axes[1].imshow(image_clahe_bgr)\n",
        "        axes[1].set_title(f\"CLAHE img- {class_name}\")\n",
        "        axes[1].axis(\"off\")\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwjT7y1dCk3D"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Define the CLAHE parameters\n",
        "clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(8, 8))\n",
        "\n",
        "# Specify the paths to the directories\n",
        "dataset_directory = \"/content/drive/MyDrive/B. Disease Grading\"\n",
        "train_images_directory = os.path.join(dataset_directory, \"1. Original Images/a. Training Set/\")\n",
        "test_images_directory = os.path.join(dataset_directory, \"1. Original Images/b. Testing Set/\")\n",
        "output_directory = os.path.join(dataset_directory, \"1. Original Images/clahe_images\")\n",
        "train_output_directory = os.path.join(output_directory, \"clahe_train_images\")\n",
        "test_output_directory = os.path.join(output_directory, \"clahe_test_images\")\n",
        "\n",
        "# Create the output directories if they don't exist\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "if not os.path.exists(train_output_directory):\n",
        "    os.makedirs(train_output_directory)\n",
        "if not os.path.exists(test_output_directory):\n",
        "    os.makedirs(test_output_directory)\n",
        "\n",
        "# Process images in train_images directory\n",
        "for filename in os.listdir(train_images_directory):\n",
        "    if filename.endswith(\".jpg\"):\n",
        "        # Read the image\n",
        "        image_path = os.path.join(train_images_directory, filename)\n",
        "        image = cv2.imread(image_path, 0)  # Load as grayscale image\n",
        "\n",
        "        # Apply CLAHE\n",
        "        image_clahe = clahe.apply(image)\n",
        "\n",
        "        # Save the CLAHE processed image\n",
        "        output_path = os.path.join(train_output_directory, filename)\n",
        "        cv2.imwrite(output_path, image_clahe)\n",
        "\n",
        "# Process images in test_images directory\n",
        "for filename in os.listdir(test_images_directory):\n",
        "    if filename.endswith(\".png\"):\n",
        "        # Read the image\n",
        "        image_path = os.path.join(test_images_directory, filename)\n",
        "        image = cv2.imread(image_path, 0)  # Load as grayscale image\n",
        "\n",
        "        # Apply CLAHE\n",
        "        image_clahe = clahe.apply(image)\n",
        "\n",
        "        # Save the CLAHE processed image\n",
        "        output_path = os.path.join(test_output_directory, filename)\n",
        "        cv2.imwrite(output_path, image_clahe)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tk248Omt09jg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuF_3BYA_QjL"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def circle_crop(img_path, sigmaX=10):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = crop_image_from_gray(img)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    height, width, depth = img.shape\n",
        "\n",
        "    x = int(width/2)\n",
        "    y = int(height/2)\n",
        "    r = np.amin((x, y))\n",
        "\n",
        "    circle_img = np.zeros((height, width), np.uint8)\n",
        "    cv2.circle(circle_img, (x, y), int(r), 1, thickness=-1)\n",
        "    img = cv2.bitwise_and(img, img, mask=circle_img)\n",
        "    img = crop_image_from_gray(img)\n",
        "    img = cv2.addWeighted(img, 4, cv2.GaussianBlur(img, (0, 0), sigmaX), -4, 128)\n",
        "    return img\n",
        "\n",
        "def crop_image_from_gray(img, tol=5):\n",
        "    if img.ndim == 2:\n",
        "        mask = img > tol\n",
        "        return img[np.ix_(mask.any(1), mask.any(0))]\n",
        "    elif img.ndim == 3:\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        mask = gray_img > tol\n",
        "\n",
        "        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]\n",
        "        if check_shape == 0:\n",
        "            return img\n",
        "        else:\n",
        "            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]\n",
        "            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]\n",
        "            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]\n",
        "            img = np.stack([img1, img2, img3], axis=-1)\n",
        "        return img\n",
        "\n",
        "# Input paths\n",
        "test_images_folder = '/content/drive/MyDrive/B. Disease Grading/1. Original Images/b. Testing Set'\n",
        "train_images_folder = '/content/drive/MyDrive/B. Disease Grading/1. Original Images/a. Training Set'\n",
        "output_test_folder = '/content/drive/MyDrive/B. Disease Grading/1. Original Images/test45'\n",
        "output_train_folder = '/content/drive/MyDrive/B. Disease Grading/1. Original Images/train45'\n",
        "\n",
        "\n",
        "# Create the output directories if they don't exist\n",
        "os.makedirs(output_test_folder, exist_ok=True)\n",
        "os.makedirs(output_train_folder, exist_ok=True)\n",
        "\n",
        "# Process test images and save cropped images\n",
        "for image_name in os.listdir(test_images_folder):\n",
        "    image_path = os.path.join(test_images_folder, image_name)\n",
        "    output_image_name = f\"{image_name.split('.')[0]}.jpg\"\n",
        "    output_image_path = os.path.join(output_test_folder, output_image_name)\n",
        "\n",
        "    # Apply circle crop\n",
        "    cropped_image = circle_crop(image_path)\n",
        "\n",
        "    # Save the cropped image\n",
        "    cv2.imwrite(output_image_path, cropped_image)\n",
        "\n",
        "# Process train images and save cropped images\n",
        "for image_name in os.listdir(train_images_folder):\n",
        "    image_path = os.path.join(train_images_folder, image_name)\n",
        "    output_image_name = f\"{image_name.split('.')[0]}.jpg\"\n",
        "    output_image_path = os.path.join(output_train_folder, output_image_name)\n",
        "\n",
        "    # Apply circle crop\n",
        "    cropped_image = circle_crop(image_path)\n",
        "\n",
        "    # Save the cropped image\n",
        "    cv2.imwrite(output_image_path, cropped_image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBFt6gOQkzTu"
      },
      "outputs": [],
      "source": [
        "\n",
        "#prev error - some image files in train_images33 cannot be read: 'NoneType' object has no attribute 'ndim'\n",
        "#so modified by just not considering that img at all and moving on to the next one\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def circle_crop(img_path, sigmaX=10):\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        print(f\"Failed to read image: {img_path}\")\n",
        "        return None\n",
        "\n",
        "    img = crop_image_from_gray(img)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    height, width, depth = img.shape\n",
        "\n",
        "    x = int(width/2)\n",
        "    y = int(height/2)\n",
        "    r = np.amin((x, y))\n",
        "\n",
        "    circle_img = np.zeros((height, width), np.uint8)\n",
        "    cv2.circle(circle_img, (x, y), int(r), 1, thickness=-1)\n",
        "    img = cv2.bitwise_and(img, img, mask=circle_img)\n",
        "    img = crop_image_from_gray(img)\n",
        "    img = cv2.addWeighted(img, 4, cv2.GaussianBlur(img, (0, 0), sigmaX), -4, 128)\n",
        "    return img\n",
        "\n",
        "def crop_image_from_gray(img, tol=5):\n",
        "    if img.ndim == 2:\n",
        "        mask = img > tol\n",
        "        return img[np.ix_(mask.any(1), mask.any(0))]\n",
        "    elif img.ndim == 3:\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        mask = gray_img > tol\n",
        "\n",
        "        check_shape = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]\n",
        "        if check_shape == 0:\n",
        "            return img\n",
        "        else:\n",
        "            img1 = img[:, :, 0][np.ix_(mask.any(1), mask.any(0))]\n",
        "            img2 = img[:, :, 1][np.ix_(mask.any(1), mask.any(0))]\n",
        "            img3 = img[:, :, 2][np.ix_(mask.any(1), mask.any(0))]\n",
        "            img = np.stack([img1, img2, img3], axis=-1)\n",
        "        return img\n",
        "\n",
        "# Input paths\n",
        "#test_images_folder = '/content/db_ret_dataset/test_images'\n",
        "train_images_folder = '/content/drive/MyDrive/B. Disease Grading/1. Original Images/a. Training Set'\n",
        "#output_test_folder = '/content/drive/MyDrive/db_ret_final_dataset/test_images55'\n",
        "output_train_folder = '/content/drive/MyDrive/B. Disease Grading/1. Original Images/train45'\n",
        "\n",
        "# Create the output directories if they don't exist\n",
        "#os.makedirs(output_test_folder, exist_ok=True)\n",
        "os.makedirs(output_train_folder, exist_ok=True)\n",
        "'''\n",
        "# Process test images and save cropped images\n",
        "for image_name in os.listdir(test_images_folder):\n",
        "    image_path = os.path.join(test_images_folder, image_name)\n",
        "    output_image_name = f\"{image_name.split('.')[0]}.png\"\n",
        "    output_image_path = os.path.join(output_test_folder, output_image_name)\n",
        "\n",
        "    print(f\"Processing image: {image_path}\")\n",
        "\n",
        "    # Check if the image file exists\n",
        "    if not os.path.isfile(image_path):\n",
        "        print(\"Image file does not exist.\")\n",
        "        continue\n",
        "\n",
        "    # Apply circle crop\n",
        "    cropped_image = circle_crop(image_path)\n",
        "\n",
        "    # Check if the cropped image is None\n",
        "    if cropped_image is None:\n",
        "        print(\"Failed to crop the image.\")\n",
        "        continue\n",
        "\n",
        "    # Save the cropped image\n",
        "    cv2.imwrite(output_image_path, cropped_image)\n",
        "'''\n",
        "# Process train images and save cropped images\n",
        "for image_name in os.listdir(train_images_folder):\n",
        "    image_path = os.path.join(train_images_folder, image_name)\n",
        "    output_image_name = f\"{image_name.split('.')[0]}.png\"\n",
        "    output_image_path = os.path.join(output_train_folder, output_image_name)\n",
        "\n",
        "    #print(f\"Processing image: {image_path}\")\n",
        "\n",
        "    # Check if the image file exists\n",
        "    if not os.path.isfile(image_path):\n",
        "        print(f\"Image file does not exist.{image_path}\")\n",
        "        continue\n",
        "\n",
        "    # Apply circle crop\n",
        "    cropped_image = circle_crop(image_path)\n",
        "\n",
        "    # Check if the cropped image is None\n",
        "    if cropped_image is None:\n",
        "        print(\"Failed to crop the image.\")\n",
        "        continue\n",
        "\n",
        "    # Save the cropped image\n",
        "    cv2.imwrite(output_image_path, cropped_image)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SZTMGVMxAwm"
      },
      "outputs": [],
      "source": [
        "img_size = (512, 512)\n",
        "num_classes = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbML3uKcj8Zz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the input image size\n",
        "input_shape = (224, 224, 3)  # Adjust the size as needed\n",
        "\n",
        "# Load and preprocess the IDRID dataset\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255.0,        # Normalize pixel values to [0, 1]\n",
        "    rotation_range=20,        # Augmentation: Randomly rotate images\n",
        "    width_shift_range=0.2,    # Augmentation: Randomly shift images horizontally\n",
        "    height_shift_range=0.2,   # Augmentation: Randomly shift images vertically\n",
        "    horizontal_flip=True,     # Augmentation: Randomly flip images horizontally\n",
        "    shear_range=0.2,          # Augmentation: Shear transformations\n",
        "    zoom_range=0.2,           # Augmentation: Randomly zoom in on images\n",
        "    fill_mode='nearest'       # Augmentation: Fill mode for image transformations\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/B. Disease Grading/1. Original Images/a. Training Set',    # Path to the training dataset\n",
        "    target_size=input_shape[:2],\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'  # Use 'categorical' for multi-class classification\n",
        ")\n",
        "\n",
        "# Define the CNN model\n",
        "model = models.Sequential([\n",
        "    # Convolutional layers\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Flatten the output for fully connected layers\n",
        "    layers.Flatten(),\n",
        "\n",
        "    # Fully connected layers\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),  # Dropout layer to reduce overfitting\n",
        "    layers.Dense(num_classes, activation='softmax')  # 'num_classes' is the number of classes in your dataset\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model architecture\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,  # Adjust the number of epochs as needed\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save the trained model for future use\n",
        "model.save('idrid_diabetic_retinopathy_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7ZhrXyA5YHv"
      },
      "outputs": [],
      "source": [
        "train_images_folder = '/content/drive/MyDrive/B. Disease Grading/1. Original Images/a. Training Set'\n",
        "test_images_folder = '/content/drive/MyDrive/B. Disease Grading/1. Original Images/a. Training Set'\n",
        "'''\n",
        "train_df = 'train33.csv'\n",
        "test_df= 'test.csv'\n",
        "'''\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/B. Disease Grading/2. Groundtruths/a. IDRiD_Disease Grading_Training Labels.csv')\n",
        "test_df= pd.read_csv('/content/drive/MyDrive/B. Disease Grading/2. Groundtruths/b. IDRiD_Disease Grading_Testing Labels.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KAGQ3PpygXS"
      },
      "outputs": [],
      "source": [
        "train_df[\"Image name\"] = train_df[\"Image name\"].apply(lambda x: x + \".jpg\")\n",
        "test_df[\"Image name\"] = train_df[\"Image name\"].apply(lambda x: x + \".jpg\")\n",
        "train_df['Retinopathy grade'] = train_df['Retinopathy grade'].astype('str')\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZ5aXW6-z6P3"
      },
      "outputs": [],
      "source": [
        "# Define num_classes\n",
        "num_classes = 10\n",
        "\n",
        "# Now you can use num_classes in your code\n",
        "if num_classes > 5:\n",
        "    print(\"There are more than 5 classes.\")\n",
        "else:\n",
        "    print(\"There are 5 or fewer classes.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qdydeLq0Acd"
      },
      "outputs": [],
      "source": [
        "img_size = (512, 512)\n",
        "num_classes = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b16kJ1B0o5n"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split= 0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "# Load and preprocess the train images\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    directory=train_images_folder,\n",
        "    x_col='Image name',\n",
        "    y_col='Retinopathy grade',\n",
        "    target_size=img_size,\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "valid_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    directory=train_images_folder,\n",
        "    x_col='Image name',\n",
        "    y_col='Retinopathy grade',\n",
        "    target_size=img_size,\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Load and preprocess the test images\n",
        "# Load and preprocess the test images\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    directory=test_images_folder,\n",
        "    x_col='Image name',\n",
        "    y_col=None,  # No class labels in the test set\n",
        "    target_size=img_size,\n",
        "    batch_size=16,\n",
        "    class_mode=None,\n",
        "    shuffle=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQMJUZi6ozWh"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    if epoch < 30:\n",
        "        return 0.001\n",
        "    elif epoch < 45:\n",
        "        return 0.0001\n",
        "    else:\n",
        "        return 0.00001\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9XThnHaDsWY",
        "outputId": "e4d1cf7e-f214-4df5-cb15-6073287904a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 1s 0us/step\n",
            "Epoch 1/79\n",
            "21/21 [==============================] - 94s 2s/step - loss: 1.6331 - accuracy: 0.4230 - val_loss: 3037.1833 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/79\n",
            "21/21 [==============================] - 40s 2s/step - loss: 1.3568 - accuracy: 0.4592 - val_loss: 142608.7500 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 1.2676 - accuracy: 0.4864 - val_loss: 12496.0078 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 1.1866 - accuracy: 0.5347 - val_loss: 2.0356 - val_accuracy: 0.0244\n",
            "Epoch 5/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 1.1004 - accuracy: 0.5559 - val_loss: 3.3989 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 1.0137 - accuracy: 0.5831 - val_loss: 3.6477 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.9905 - accuracy: 0.5861 - val_loss: 2.7961 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/79\n",
            "21/21 [==============================] - 43s 2s/step - loss: 0.9724 - accuracy: 0.6012 - val_loss: 2.2972 - val_accuracy: 0.4268\n",
            "Epoch 9/79\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.9374 - accuracy: 0.6314 - val_loss: 3.8587 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.9346 - accuracy: 0.5831 - val_loss: 6.5969 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/79\n",
            "21/21 [==============================] - 42s 2s/step - loss: 0.9662 - accuracy: 0.6133 - val_loss: 6.8536 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/79\n",
            "21/21 [==============================] - 42s 2s/step - loss: 0.8631 - accuracy: 0.6375 - val_loss: 3.9955 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/79\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.9152 - accuracy: 0.6314 - val_loss: 3.6717 - val_accuracy: 0.4268\n",
            "Epoch 14/79\n",
            "21/21 [==============================] - 42s 2s/step - loss: 0.8733 - accuracy: 0.6405 - val_loss: 2.4960 - val_accuracy: 0.4268\n",
            "Epoch 15/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.9154 - accuracy: 0.6042 - val_loss: 2.4339 - val_accuracy: 0.0488\n",
            "Epoch 16/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.8662 - accuracy: 0.6344 - val_loss: 3.2110 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.7801 - accuracy: 0.6767 - val_loss: 3.7832 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/79\n",
            "21/21 [==============================] - 42s 2s/step - loss: 0.8240 - accuracy: 0.6616 - val_loss: 1.6553 - val_accuracy: 0.0122\n",
            "Epoch 19/79\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.8020 - accuracy: 0.6556 - val_loss: 3.1929 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/79\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.8637 - accuracy: 0.6344 - val_loss: 3.8831 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.7521 - accuracy: 0.6737 - val_loss: 3.2830 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/79\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.7825 - accuracy: 0.6737 - val_loss: 3.4650 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.9010 - accuracy: 0.6012 - val_loss: 4.4274 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.8112 - accuracy: 0.6314 - val_loss: 5.0412 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.7762 - accuracy: 0.6616 - val_loss: 3.3762 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/79\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.7450 - accuracy: 0.6647 - val_loss: 6.5792 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.6830 - accuracy: 0.7130 - val_loss: 6.9953 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/79\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.7639 - accuracy: 0.6828 - val_loss: 11.1859 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/79\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.7967 - accuracy: 0.6737 - val_loss: 5.6554 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/79\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.7312 - accuracy: 0.6737 - val_loss: 7.1197 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.6743 - accuracy: 0.7039 - val_loss: 6.8571 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.6635 - accuracy: 0.7221 - val_loss: 5.4533 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/79\n",
            "21/21 [==============================] - 42s 2s/step - loss: 0.6457 - accuracy: 0.7069 - val_loss: 3.2732 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.7247 - accuracy: 0.6888 - val_loss: 5.9225 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.7190 - accuracy: 0.6888 - val_loss: 9.4482 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/79\n",
            "21/21 [==============================] - 42s 2s/step - loss: 0.7481 - accuracy: 0.6798 - val_loss: 5.7400 - val_accuracy: 0.0122\n",
            "Epoch 37/79\n",
            "21/21 [==============================] - 42s 2s/step - loss: 0.6716 - accuracy: 0.7009 - val_loss: 10.9145 - val_accuracy: 0.0122\n",
            "Epoch 38/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.6545 - accuracy: 0.6949 - val_loss: 11.4539 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.6833 - accuracy: 0.7221 - val_loss: 11.2247 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.6408 - accuracy: 0.7372 - val_loss: 8.0184 - val_accuracy: 0.0122\n",
            "Epoch 41/79\n",
            "21/21 [==============================] - 42s 2s/step - loss: 0.6352 - accuracy: 0.7281 - val_loss: 11.6355 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.6480 - accuracy: 0.7251 - val_loss: 7.5323 - val_accuracy: 0.1829\n",
            "Epoch 43/79\n",
            "21/21 [==============================] - 42s 2s/step - loss: 0.6373 - accuracy: 0.7553 - val_loss: 9.2581 - val_accuracy: 0.1098\n",
            "Epoch 44/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.6457 - accuracy: 0.7402 - val_loss: 3.7561 - val_accuracy: 0.2805\n",
            "Epoch 45/79\n",
            "21/21 [==============================] - 42s 2s/step - loss: 0.5971 - accuracy: 0.7341 - val_loss: 4.0432 - val_accuracy: 0.1829\n",
            "Epoch 46/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.6365 - accuracy: 0.7432 - val_loss: 5.9590 - val_accuracy: 0.1829\n",
            "Epoch 47/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.6163 - accuracy: 0.7402 - val_loss: 7.0862 - val_accuracy: 0.0488\n",
            "Epoch 48/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.5882 - accuracy: 0.7885 - val_loss: 6.2717 - val_accuracy: 0.2683\n",
            "Epoch 49/79\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.6438 - accuracy: 0.7432 - val_loss: 3.9934 - val_accuracy: 0.2683\n",
            "Epoch 50/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.6500 - accuracy: 0.7190 - val_loss: 4.1452 - val_accuracy: 0.2195\n",
            "Epoch 51/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.6138 - accuracy: 0.7311 - val_loss: 3.4115 - val_accuracy: 0.3049\n",
            "Epoch 52/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.5986 - accuracy: 0.7492 - val_loss: 3.5194 - val_accuracy: 0.3049\n",
            "Epoch 53/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.6816 - accuracy: 0.7402 - val_loss: 3.3625 - val_accuracy: 0.3537\n",
            "Epoch 54/79\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.6033 - accuracy: 0.7674 - val_loss: 2.8122 - val_accuracy: 0.3537\n",
            "Epoch 55/79\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.6198 - accuracy: 0.7613 - val_loss: 3.0188 - val_accuracy: 0.3293\n",
            "Epoch 56/79\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.5607 - accuracy: 0.7553 - val_loss: 3.2952 - val_accuracy: 0.3537\n",
            "Epoch 57/79\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.6441 - accuracy: 0.7341 - val_loss: 2.0618 - val_accuracy: 0.3537\n",
            "Epoch 58/79\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.6047 - accuracy: 0.7613 - val_loss: 3.1300 - val_accuracy: 0.3415\n",
            "Epoch 59/79\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.5965 - accuracy: 0.7462 - val_loss: 4.8757 - val_accuracy: 0.3659\n",
            "Epoch 60/79\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.5607 - accuracy: 0.7613 - val_loss: 4.4154 - val_accuracy: 0.3415\n",
            "Epoch 61/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.5482 - accuracy: 0.7734 - val_loss: 3.3589 - val_accuracy: 0.3780\n",
            "Epoch 62/79\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.4687 - accuracy: 0.8097 - val_loss: 5.4223 - val_accuracy: 0.3537\n",
            "Epoch 63/79\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.5241 - accuracy: 0.7613 - val_loss: 2.4980 - val_accuracy: 0.4146\n",
            "Epoch 64/79\n",
            "21/21 [==============================] - 40s 2s/step - loss: 0.5550 - accuracy: 0.8036 - val_loss: 3.0177 - val_accuracy: 0.3659\n",
            "Epoch 65/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.4964 - accuracy: 0.8066 - val_loss: 2.4923 - val_accuracy: 0.4024\n",
            "Epoch 66/79\n",
            "21/21 [==============================] - 41s 2s/step - loss: 0.4089 - accuracy: 0.8489 - val_loss: 2.3587 - val_accuracy: 0.3049\n",
            "Epoch 67/79\n",
            "21/21 [==============================] - 42s 2s/step - loss: 0.5042 - accuracy: 0.7946 - val_loss: 3.1372 - val_accuracy: 0.2561\n",
            "Epoch 68/79\n",
            "21/21 [==============================] - 42s 2s/step - loss: 0.4793 - accuracy: 0.8187 - val_loss: 4.1127 - val_accuracy: 0.3049\n",
            "Epoch 69/79\n",
            "21/21 [==============================] - 42s 2s/step - loss: 0.5152 - accuracy: 0.7915 - val_loss: 3.5337 - val_accuracy: 0.3049\n",
            "Epoch 70/79\n",
            "21/21 [==============================] - 42s 2s/step - loss: 0.4404 - accuracy: 0.8187 - val_loss: 2.1530 - val_accuracy: 0.3293\n",
            "Epoch 71/79\n",
            "21/21 [==============================] - 42s 2s/step - loss: 0.4279 - accuracy: 0.8338 - val_loss: 2.8929 - val_accuracy: 0.4146\n",
            "Epoch 72/79\n",
            "21/21 [==============================] - 42s 2s/step - loss: 0.5706 - accuracy: 0.7764 - val_loss: 13.5338 - val_accuracy: 0.4146\n",
            "Epoch 73/79\n",
            "21/21 [==============================] - 43s 2s/step - loss: 0.5938 - accuracy: 0.7674 - val_loss: 4.9979 - val_accuracy: 0.3171\n",
            "Epoch 74/79\n",
            "21/21 [==============================] - 43s 2s/step - loss: 0.4463 - accuracy: 0.8157 - val_loss: 7.2416 - val_accuracy: 0.3171\n",
            "Epoch 75/79\n",
            "21/21 [==============================] - 43s 2s/step - loss: 0.4684 - accuracy: 0.8278 - val_loss: 5.3713 - val_accuracy: 0.3293\n",
            "Epoch 76/79\n",
            "21/21 [==============================] - 44s 2s/step - loss: 0.4388 - accuracy: 0.8489 - val_loss: 2.7347 - val_accuracy: 0.3537\n",
            "Epoch 77/79\n",
            "21/21 [==============================] - 44s 2s/step - loss: 0.4651 - accuracy: 0.8097 - val_loss: 2.5045 - val_accuracy: 0.3902\n",
            "Epoch 78/79\n",
            "21/21 [==============================] - 44s 2s/step - loss: 0.4547 - accuracy: 0.8338 - val_loss: 7.2949 - val_accuracy: 0.3780\n",
            "Epoch 79/79\n",
            "21/21 [==============================] - 44s 2s/step - loss: 0.4780 - accuracy: 0.7976 - val_loss: 5.0580 - val_accuracy: 0.3415\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a1455093d30>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load ResNet50 model\n",
        "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(img_size[0], img_size[1], 3))\n",
        "\n",
        "#model architecture\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(train_generator, epochs=79, validation_data=valid_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Eh_QT0MEo3l"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1_m7kPbjpZ7KW4eXFRl2oq_x9CTLqwG6k",
      "authorship_tag": "ABX9TyMkwuCWliOkWHZGRYPS3Ta1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}